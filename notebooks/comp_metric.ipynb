{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Empirically check how evaluation works\n",
    "Code taken from this [notebook](https://www.kaggle.com/code/kurupical/easy-examples-to-understand-competition-metric) made by [kurupical](https://www.kaggle.com/kurupical)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.testing import assert_index_equal\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "tolerances = {\n",
    "    \"challenge\": [0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "    \"play\": [0.15, 0.20, 0.25, 0.30, 0.35],\n",
    "    \"throwin\": [0.15, 0.20, 0.25, 0.30, 0.35],\n",
    "}\n",
    "\n",
    "\n",
    "def filter_detections(\n",
    "        detections: pd.DataFrame, intervals: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Drop detections not inside a scoring interval.\"\"\"\n",
    "    detection_time = detections.loc[:, 'time'].sort_values().to_numpy()\n",
    "    intervals = intervals.to_numpy()\n",
    "    is_scored = np.full_like(detection_time, False, dtype=bool)\n",
    "\n",
    "    i, j = 0, 0\n",
    "    while i < len(detection_time) and j < len(intervals):\n",
    "        time = detection_time[i]\n",
    "        int_ = intervals[j]\n",
    "\n",
    "        # If the detection is prior in time to the interval, go to the next detection.\n",
    "        if time < int_.left:\n",
    "            i += 1\n",
    "        # If the detection is inside the interval, keep it and go to the next detection.\n",
    "        elif time in int_:\n",
    "            is_scored[i] = True\n",
    "            i += 1\n",
    "        # If the detection is later in time, go to the next interval.\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "    return detections.loc[is_scored].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def match_detections(\n",
    "        tolerance: float, ground_truths: pd.DataFrame, detections: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Match detections to ground truth events. Arguments are taken from a common event x tolerance x video evaluation group.\"\"\"\n",
    "    detections_sorted = detections.sort_values('score', ascending=False).dropna()\n",
    "\n",
    "    is_matched = np.full_like(detections_sorted['event'], False, dtype=bool)\n",
    "    gts_matched = set()\n",
    "    for i, det in enumerate(detections_sorted.itertuples(index=False)):\n",
    "        best_error = tolerance\n",
    "        best_gt = None\n",
    "\n",
    "        for gt in ground_truths.itertuples(index=False):\n",
    "            error = abs(det.time - gt.time)\n",
    "            if error < best_error and not gt in gts_matched:\n",
    "                best_gt = gt\n",
    "                best_error = error\n",
    "\n",
    "        if best_gt is not None:\n",
    "            is_matched[i] = True\n",
    "            gts_matched.add(best_gt)\n",
    "\n",
    "    detections_sorted['matched'] = is_matched\n",
    "\n",
    "    return detections_sorted\n",
    "\n",
    "\n",
    "def precision_recall_curve(\n",
    "        matches: np.ndarray, scores: np.ndarray, p: int\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    if len(matches) == 0:\n",
    "        return [1], [0], []\n",
    "\n",
    "    # Sort matches by decreasing confidence\n",
    "    idxs = np.argsort(scores, kind='stable')[::-1]\n",
    "    scores = scores[idxs]\n",
    "    matches = matches[idxs]\n",
    "\n",
    "    distinct_value_indices = np.where(np.diff(scores))[0]\n",
    "    threshold_idxs = np.r_[distinct_value_indices, matches.size - 1]\n",
    "    thresholds = scores[threshold_idxs]\n",
    "\n",
    "    # Matches become TPs and non-matches FPs as confidence threshold decreases\n",
    "    tps = np.cumsum(matches)[threshold_idxs]\n",
    "    fps = np.cumsum(~matches)[threshold_idxs]\n",
    "\n",
    "    precision = tps / (tps + fps)\n",
    "    precision[np.isnan(precision)] = 0\n",
    "    recall = tps / p  # total number of ground truths might be different than total number of matches\n",
    "\n",
    "    # Stop when full recall attained and reverse the outputs so recall is non-increasing.\n",
    "    last_ind = tps.searchsorted(tps[-1])\n",
    "    sl = slice(last_ind, None, -1)\n",
    "\n",
    "    # Final precision is 1 and final recall is 0\n",
    "    return np.r_[precision[sl], 1], np.r_[recall[sl], 0], thresholds[sl]\n",
    "\n",
    "\n",
    "def average_precision_score(matches: np.ndarray, scores: np.ndarray, p: int) -> float:\n",
    "    precision, recall, thresholds = precision_recall_curve(matches, scores, p)\n",
    "    # Compute step integral\n",
    "    return -np.sum(np.diff(recall) * np.array(precision)[:-1])\n",
    "\n",
    "\n",
    "def event_detection_ap(\n",
    "        solution: pd.DataFrame,\n",
    "        submission: pd.DataFrame,\n",
    "        tolerances: Dict[str, float],\n",
    ") -> float:\n",
    "\n",
    "    assert_index_equal(solution.columns, pd.Index(['video_id', 'time', 'event']))\n",
    "    assert_index_equal(submission.columns, pd.Index(['video_id', 'time', 'event', 'score']))\n",
    "\n",
    "    # Extract scoring intervals.\n",
    "    intervals = (\n",
    "        solution\n",
    "        .query(\"event in ['start', 'end']\")\n",
    "        .assign(interval=lambda x: x.groupby(['video_id', 'event']).cumcount())\n",
    "        .pivot(index='interval', columns=['video_id', 'event'], values='time')\n",
    "        .stack('video_id')\n",
    "        .swaplevel()\n",
    "        .sort_index()\n",
    "        .loc[:, ['start', 'end']]\n",
    "        .apply(lambda x: pd.Interval(*x, closed='both'), axis=1)\n",
    "    )\n",
    "\n",
    "    # Extract ground-truth events.\n",
    "    ground_truths = (\n",
    "        solution\n",
    "        .query(\"event not in ['start', 'end']\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Map each event class to its prevalence (needed for recall calculation)\n",
    "    class_counts = ground_truths.value_counts('event').to_dict()\n",
    "\n",
    "    # Create table for detections with a column indicating a match to a ground-truth event\n",
    "    detections = submission.assign(matched = False)\n",
    "\n",
    "    # Remove detections outside of scoring intervals\n",
    "    detections_filtered = []\n",
    "    for (det_group, dets), (int_group, ints) in zip(\n",
    "        detections.groupby('video_id'), intervals.groupby('video_id')\n",
    "    ):\n",
    "        assert det_group == int_group\n",
    "        detections_filtered.append(filter_detections(dets, ints))\n",
    "    detections_filtered = pd.concat(detections_filtered, ignore_index=True)\n",
    "\n",
    "    # Create table of event-class x tolerance x video_id values\n",
    "    aggregation_keys = pd.DataFrame(\n",
    "        [(ev, tol, vid)\n",
    "         for ev in tolerances.keys()\n",
    "         for tol in tolerances[ev]\n",
    "         for vid in ground_truths['video_id'].unique()],\n",
    "        columns=['event', 'tolerance', 'video_id'],\n",
    "    )\n",
    "\n",
    "    # Create match evaluation groups: event-class x tolerance x video_id\n",
    "    detections_grouped = (\n",
    "        aggregation_keys\n",
    "        .merge(detections_filtered, on=['event', 'video_id'], how='left')\n",
    "        .groupby(['event', 'tolerance', 'video_id'])\n",
    "    )\n",
    "    ground_truths_grouped = (\n",
    "        aggregation_keys\n",
    "        .merge(ground_truths, on=['event', 'video_id'], how='left')\n",
    "        .groupby(['event', 'tolerance', 'video_id'])\n",
    "    )\n",
    "\n",
    "    # Match detections to ground truth events by evaluation group\n",
    "    detections_matched = []\n",
    "    for key in aggregation_keys.itertuples(index=False):\n",
    "        dets = detections_grouped.get_group(key)\n",
    "        gts = ground_truths_grouped.get_group(key)\n",
    "        detections_matched.append(\n",
    "            match_detections(dets['tolerance'].iloc[0], gts, dets)\n",
    "        )\n",
    "    detections_matched = pd.concat(detections_matched)\n",
    "    print(f\"detection_matched: \\n {detections_matched}\")\n",
    "\n",
    "    # Compute AP per event x tolerance group\n",
    "    event_classes = ground_truths['event'].unique()\n",
    "    ap_table = (\n",
    "        detections_matched\n",
    "        .query(\"event in @event_classes\")\n",
    "        .groupby(['event', 'tolerance']).apply(\n",
    "        lambda group: average_precision_score(\n",
    "        group['matched'].to_numpy(),\n",
    "                group['score'].to_numpy(),\n",
    "                class_counts[group['event'].iat[0]],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(f\"ap_table: \\n {ap_table}\")\n",
    "\n",
    "    # Average over tolerances, then over event classes\n",
    "    mean_ap = ap_table.groupby('event').mean().mean()\n",
    "\n",
    "    return mean_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "  video_id  time      event\n0    case1     1      start\n1    case1     2       play\n2    case1     3        end\n3    case1    11      start\n4    case1    12  challenge\n5    case1    13        end\n6    case1    21      start\n7    case1    22    throwin\n8    case1    23        end",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>time</th>\n      <th>event</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>case1</td>\n      <td>1</td>\n      <td>start</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>case1</td>\n      <td>2</td>\n      <td>play</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>case1</td>\n      <td>3</td>\n      <td>end</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>case1</td>\n      <td>11</td>\n      <td>start</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>case1</td>\n      <td>12</td>\n      <td>challenge</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>case1</td>\n      <td>13</td>\n      <td>end</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>case1</td>\n      <td>21</td>\n      <td>start</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>case1</td>\n      <td>22</td>\n      <td>throwin</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>case1</td>\n      <td>23</td>\n      <td>end</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_case1 = pd.DataFrame({\n",
    "    \"video_id\": [\"case1\"] * 9,\n",
    "    \"time\": [1, 2, 3] + [11, 12, 13] + [21, 22, 23],\n",
    "    \"event\": [\"start\", \"play\", \"end\"] + [\"start\", \"challenge\", \"end\"] + [\"start\", \"throwin\", \"end\"]\n",
    "})\n",
    "df_case1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detection_matched: \n",
      "         event  tolerance video_id  time  score  matched\n",
      "0   challenge       0.30    case1    12      1     True\n",
      "1   challenge       0.40    case1    12      1     True\n",
      "2   challenge       0.50    case1    12      1     True\n",
      "3   challenge       0.60    case1    12      1     True\n",
      "4   challenge       0.70    case1    12      1     True\n",
      "5        play       0.15    case1     2      1     True\n",
      "6        play       0.20    case1     2      1     True\n",
      "7        play       0.25    case1     2      1     True\n",
      "8        play       0.30    case1     2      1     True\n",
      "9        play       0.35    case1     2      1     True\n",
      "10    throwin       0.15    case1    22      1     True\n",
      "11    throwin       0.20    case1    22      1     True\n",
      "12    throwin       0.25    case1    22      1     True\n",
      "13    throwin       0.30    case1    22      1     True\n",
      "14    throwin       0.35    case1    22      1     True\n",
      "ap_table: \n",
      " event      tolerance\n",
      "challenge  0.30         1.0\n",
      "           0.40         1.0\n",
      "           0.50         1.0\n",
      "           0.60         1.0\n",
      "           0.70         1.0\n",
      "play       0.15         1.0\n",
      "           0.20         1.0\n",
      "           0.25         1.0\n",
      "           0.30         1.0\n",
      "           0.35         1.0\n",
      "throwin    0.15         1.0\n",
      "           0.20         1.0\n",
      "           0.25         1.0\n",
      "           0.30         1.0\n",
      "           0.35         1.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred1 = pd.DataFrame({\n",
    "    \"video_id\": [\"case1\"] * 3,\n",
    "    \"time\": [2, 12, 22],\n",
    "    \"event\": [\"play\", \"challenge\", \"throwin\"],\n",
    "    \"score\": [1, 1, 1]\n",
    "})\n",
    "# df_pred1\n",
    "event_detection_ap(solution=df_case1, submission=df_pred1, tolerances=tolerances)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detection_matched: \n",
      "         event  tolerance video_id  time  score  matched\n",
      "0   challenge       0.30    case1  12.0      1     True\n",
      "1   challenge       0.40    case1  12.0      1     True\n",
      "2   challenge       0.50    case1  12.0      1     True\n",
      "3   challenge       0.60    case1  12.0      1     True\n",
      "4   challenge       0.70    case1  12.0      1     True\n",
      "5        play       0.15    case1   1.7      1    False\n",
      "6        play       0.20    case1   1.7      1    False\n",
      "7        play       0.25    case1   1.7      1    False\n",
      "8        play       0.30    case1   1.7      1    False\n",
      "9        play       0.35    case1   1.7      1     True\n",
      "10    throwin       0.15    case1  22.0      1     True\n",
      "11    throwin       0.20    case1  22.0      1     True\n",
      "12    throwin       0.25    case1  22.0      1     True\n",
      "13    throwin       0.30    case1  22.0      1     True\n",
      "14    throwin       0.35    case1  22.0      1     True\n",
      "ap_table: \n",
      " event      tolerance\n",
      "challenge  0.30         1.0\n",
      "           0.40         1.0\n",
      "           0.50         1.0\n",
      "           0.60         1.0\n",
      "           0.70         1.0\n",
      "play       0.15        -0.0\n",
      "           0.20        -0.0\n",
      "           0.25        -0.0\n",
      "           0.30        -0.0\n",
      "           0.35         1.0\n",
      "throwin    0.15         1.0\n",
      "           0.20         1.0\n",
      "           0.25         1.0\n",
      "           0.30         1.0\n",
      "           0.35         1.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.7333333333333334"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average Precision = (1 + 1 + 0.2) / 3 = 0.7333\n",
    "#   play: 0.2 (n_prediction=5, n_correct=1(failed to predict torelances [0.15, 0.20, 0.25, 0.30]))\n",
    "#   challenge: 1\n",
    "#   throwin: 1\n",
    "\n",
    "# [Example] In case of event \"play\" or \"throwin\", if answer is time=2:\n",
    "#   get 1.0(5/5) score if you predict play time 1.85 ~ 2.15\n",
    "#   get 0.8(4/5) score if you predict play time 1.80 ~ 1.85 or 2.15 ~ 2.20\n",
    "#   get 0.6(3/5) score if you predict play time 1.75 ~ 1.80 or 2.20 ~ 2.25\n",
    "#   get 0.4(2/5) score if you predict play time 1.70 ~ 1.75 or 2.25 ~ 2.30\n",
    "#   get 0.2(1/5) score if you predict play time 1.65 ~ 1.70 or 2.30 ~ 2.35\n",
    "#   get 0.0(0/5) score if you predict play time ~ 1.65 or 2.35 ~\n",
    "\n",
    "df_pred1 = pd.DataFrame({\n",
    "    \"video_id\": [\"case1\"] * 3,\n",
    "    \"time\": [1.7, 12, 22],\n",
    "    \"event\": [\"play\", \"challenge\", \"throwin\"],\n",
    "    \"score\": [1, 1, 1]\n",
    "})\n",
    "event_detection_ap(solution=df_case1, submission=df_pred1, tolerances=tolerances)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}